\newpage % Rozdziały zaczynamy od nowej strony.
\section{Opis wybranych elementów implementacji}
Rozdział poświęcony będzie najważniejszym elementom implementacji, przedstawione w nim zostaną kody źródłowe struktur, których opis teoretyczny czytelnik mógł znaleźć w poprzednim rozdziale. Przedstawienie wszystkich składników rozwiązania można uznać za niecelowe i zbędnie wydłużające pracę dlatego uwaga poświęcona zostanie jedynie krytycznym elementom z punktu widzenia zamierzonej funkcjonalności i realizacji postawionego zadania.
\par Prezentacje można podzielić na trzy oddzielne fragmenty, w pierwszej pokazana zostanie implementacja algorytmu DMC wraz z używanym w trakcie eksperymentów obiektem regulacji. Następnie czytelnik pozna najważniejsze elementy dotyczące implementacji sieci neuronowej ze szczególnym uwzględnieniem algorytmu OBD, któremu poświęcony jest ostatni podrozdział. Całość rozwiązania zaimplementowana została z wykorzystaniem języka Python 3.6 i korzysta z kilku podstawowych zewnętrznych bibliotek. Za kluczową ze względu na charakter pracy należy uznać bibliotekę NumPy pozwalająca na obsługę wielowymiarowych tablic i macierzy, a także udostępniającą zbiór funkcji matematycznych wysokiego poziomu do obsługi tych tablic. W trakcie implementacji nieocenione okazały się opracowania \cite{nielsen2015} oraz \cite{wawrzynski2019}. 

\subsection{Algorytm DMC z obiektem regulacji}
Algorytm regulacji predykcyjnej zaimplementowany został jako klasa DMC, której strukturę widzimy w poniższym kodzie: 
\begin{addmargin}[10mm]{0mm}
\begin{lstlisting}[
    language=Python,
    numbers=left,
    firstnumber=1,
    caption={Klasa DMC},
    aboveskip=10pt  
]
class DMC(object):
 def __init__(self, n_u, n, S_list, psi_const, lambda_const):		
  #horyzont sterwania 
  self.n_u = n_u
  #horyzont predykcji
  self.n = n 
  #horyzont dynamiki obiektu - ile s wyzaczamy (10, 20, 30)
  self.d = len(S_list)

  self.Y_k = np.zeros(n)
  #wektor wartosci zadanej - stala na calym horyzoncie N
  self.Y_zad = np.zeros(n)
  #wektor opisujacy trajektorie sygnalu wyjsciowego 
  self.Y_hat = np.zeros(n)

  #wektor wyznaczanych przyrostow wartosci sterowania 
  self.U_delta = np.zeros(n_u)

  #wektor przeszlych przyrostow sterowania
  self.U_delta_P = np.zeros(len(S_list)-1)
		
  #macierze wyznaczane w trakcie inicjalizacji klasy
  self.Psi = np.diag([psi_const for i in range(n)])
  self.Lambda = np.diag([lambda_const for i in range(n_u)])
  self.M_p = self.init_M_p(S_list, n)
  self.M = self.init_M(S_list, n, n_u)
  self.K = inv(self.M.T @ self.Psi @ self.M + self.Lambda) \ 
					@ self.M.T @ self.Psi
\end{lstlisting}
\end{addmargin}

Klasa inicjalizowana jest zgodnie z opisem teoretycznym w sekcji 3.1.1. z wykorzystaniem wektora odpowiedzi skokowej układu reprezentowanego przez argument \emph{ S{\_}list }. Warto zwrócić tutaj uwagę na linię 27-28 Listingu 1, która stanowi bezpośrednią implementację macierzy \(K\) przedstawionej za pomocą równania (9).
\par W dalszej części występują funkcje pomocnicze wykorzystywane do tworzenia macierzy \(M\) oraz \(M^P\), które ze względu na ich trywialność zostaną pominięte. Kluczowym elementem całej klasy ze względu na działanie algorytmu jest funkcja wyznaczająca wektor optymalnych przyrostów sterowania w każdej iteracji algorytmu. Implementację funkcji prezentuje Listing 2, gdzie w liniach 4-5 zaimplementowano równanie (8). W każdej iteracji algorytmu zwracaną jest tylko wartość \(\Delta u(k|k) \), której odpowiada zwracana przez funkcję zmienna \emph{delta{\_}u}.

\begin{addmargin}[6mm]{0mm}
\begin{lstlisting}[
    language=Python,
    numbers=left,
    firstnumber=1,
    caption={Funcja wyznaczająca wektor przyrostów sterowania},
    aboveskip=10pt
]
def calculate_U_delta(self, Y_current): 

	self.Y_k.fill(Y_current)
	self.U_delta = self.K @ (self.Y_zad - self.Y_k - \
		 (self.M_p @ self.U_delta_P))
	delta_u = self.U_delta[0]
	self.update_U_delta_P(delta_u)

	return delta_u
\end{lstlisting}
\end{addmargin}

\par Obiekt regulacji zaimplementowany został w postaci klasy \emph{SimObject} gdzie istotę działania obiektu prezentuje funkcja zwracająca wyjście obiektu na podstawie wartości sterowania, w każdej kolejnej iteracji. Listing 3 prezentuje funkcję \emph{make{\_}simulation{\_}step}, w której należy zwrócić uwagę na linie 8-9, w których widzimy zaimplementowane równanie (10). 
\begin{addmargin}[10mm]{0mm}
\begin{lstlisting}[
    language=Python,
    numbers=left,
    firstnumber=1,
    caption={Funcja wyznaczająca wektor przyrostów sterowania},
    aboveskip=10pt
]
def make_simulation_step(self, u, y_zad):
		
 u_lag1=self.get_lag_u(self.TD+1)
 u_lag2=self.get_lag_u(self.TD+2)
 y_lag1=self.get_lag_y(1)
 y_lag2=self.get_lag_y(2)

 y_current=self.b_1*u_lag1 + self.b_2*u_lag2 - \
  self.a_1*y_lag1 - self.a_2*y_lag2
 self.y_list = np.append(self.y_list, y_current)
 self.u_list = np.append(self.u_list, u)
 self.y_zad_list = np.append(self.y_zad_list, y_zad)
 return y_current
\end{lstlisting}
\end{addmargin}
 
\subsection{Architektura sieci neuronowej}
Implementacja sieci neuronowej przedstawiona została w postaci klasy \emph{Network}, której konstruktor widzimy w Listingu 4. Struktura sieci składa się z listy macierzy \emph{weights} (linia 7) oraz \emph{biases} (linia 6) odpowiadających kolejnym wagom oraz wyrazom wolnym dla połączeń między neuronami sąsiadujących warstw. Natomiast lista macierzy \emph{saliencies} o tej samej wymiarowości co \emph{weights} zawierają współczynniki asymetrii wyznaczane w trakcie działania algorytmu OBD. Zmienna \emph{mask} wykorzystywana jest przez algorytm OBD do zamrożenia danych wag w trakcie ponownego uczenia sieci. Na końcu inicjalizacji ustalany jest parametr \emph{cost{\_}delta{\_}epsilon} stanowiący graniczną wartość gradientu funkcji celu poniżej, której proces uczenia sieci zostaje zakończony.    
\begin{addmargin}[10mm]{0mm}
\begin{lstlisting}[
    language=Python,
    numbers=left,
    firstnumber=1,
    caption={Klasa Network},
    aboveskip=10pt
]
class Network(object):
 def __init__(self, sizes):
 
  self.num_layers = len(sizes)
  self.sizes = sizes
  self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
  self.weights = [np.random.randn(y, x)
                  for x, y in zip(sizes[:-1], sizes[1:])]
  self.saliencies = [np.zeros((y, x))
                  for x, y in zip(sizes[:-1], sizes[1:])]
  self.mask = [np.ones((y, x))
                  for x, y in zip(sizes[:-1], sizes[1:])]

  self.cost_delta_epsilon = 0.000005
\end{lstlisting}
\end{addmargin}
\par Główną część implementacji sieci neuronowej stanowią funkcje wykorzystywane w trakcie procesu trenowania sieci. Jest to odpowiednio funkcja \emph{SGD} implementująca metodę stochastycznego najszybszego spadku (SGD), jedną z najczęściej stosowanych metod iteracyjnej optymalizacji funkcji celu. Metoda to korzysta z funkcji \emph{update{\_}mini{\_}batch}, która to odpowiada za zastosowanie metody wstecznej propagacji dla każdego z przykładów uczących występujących w pojedynczej próbce wyznaczanej przez SGD. Kluczowym dla działania wymienionych funkcji jest implementacja metody wstecznej propagacji dla pojedynczego przykładu uczącego. Teoretyczne założenia używanej metody przedstawione zostały w sekcji 3.2.2, a jej implementacje widzimy na Listingu 5. Poprzednie funkcje nie zostały tutaj zaprezentowane gdyż nie prezentują głównej idei wstecznej propagacji, a jedynie z niej korzystają, co mogłoby niepotrzebnie wydłużyć całość wywodu. W początkowej części (linie 2-3) tworzone są wektory odpowiadające pochodnych cząstkowych \( \frac{\partial q}{\partial w_{jk}^l} \) oraz \( \frac{\partial q}{\partial b_j^l} \). Następnie (linie 5-12) następuje jednokrotne przejście sieci w przód (\emph{ang. feedforward}) czyli wyliczenie wyjścia sieci na podstawie wektora wejściowego \emph{x}. Główna zasada działania wstecznej propagacji znajduje odzwierciedlenie w kolejnych liniach kodu. Należy zwrócić tutaj szczególną uwagę na linie 15-16, gdzie wykorzystane zostało równanie (15) stanowiące warunek początkowy dla algorytmu. Następnie w pętli następuje wsteczna propagacja między warstwami opisania równaniem (16), które znajduje bezpośrednie odzwierciedlenie w linijce 23.
\begin{addmargin}[10mm]{0mm}
\begin{lstlisting}[
    language=Python,
    numbers=left,
    firstnumber=1,
    caption={Funkcja wstecznej propagacji},
    aboveskip=10pt
]
def backprop(self, x, y):
 nabla_b = [np.zeros(b.shape) for b in self.biases]
 nabla_w = [np.zeros(w.shape) for w in self.weights]
 # jednokrotne wyznaczenie wyjscia sieci dla wjescia x
 activation = x
 activations = [x] 
 zs = []
 for b, w in zip(self.biases, self.weights):
   z = np.dot(w, activation)+b
   zs.append(z)
   activation = sigmoid(z)
   activations.append(activation)
 # wsteczna propagacja
 # warunek poczatkowy
 delta = self.cost_derivative(activations[-1], y) * \
            sigmoid_prime(zs[-1])
 nabla_b[-1] = delta
 nabla_w[-1] = np.dot(delta, activations[-2].transpose())
 # iteracja od konca po warstwach
 for l in range(2, self.num_layers):
   z = zs[-l]
   sp = sigmoid_prime(z)
   delta = np.dot(self.weights[-l+1].transpose(), delta) * sp
   nabla_b[-l] = delta
   nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())
 return (nabla_b, nabla_w)
\end{lstlisting}
\end{addmargin}

\par Pozostała część implementacji obejmuje funkcje aktywacji, funkcje wyliczająca wyjście sieci na podstawie zadanego wektora wejściowego, a także wszelkie niezbędne funkcje pomocnicze. Sposób ich implementacji nie jest kluczowy dla zrozumienia całości pracy, a może być w łatwy sposób odtworzony na podstawie opisu teoretycznego. Z punktu widzenia czytelnika warto jednak przyjrzeć się ostatniej części klasy \emph{Network}, która zawiera implementację algorytmu redukcji sieci OBD.

\subsection{Algorytm OBD}
Algorytm OBD zaimplementowany został jako część klasy \emph{Network} na podstawie opisu teoretycznego zawartego w sekcji 3.2.3. i składa się z trzech głównych funkcji \emph{OBD}, \emph{backpropOBD} oraz \emph{cut{\_}weights}. Pierwsza z nich przedstawiona została na Listingu 6 i odpowiada za implementację procedury algorytmu OBD  przedstawionej w opisie teoretycznym. Kluczowa dla działania algorytmu jest pętla rozpoczynająca się w linii 8, która kolejno wywołuje metodę wstecznej propagacji dla drugich pochodnych. Po iteracji dla wszystkich przykładów uczących obliczana jest wartość współczynników asymetrii i następnie w linii 16 wywoływana jest funkcja \emph{cut{\_}weights} odpowiedzialna za przycinanie sieci. Po redukcji sieć jest ponownie uczona z wykorzystaniem metody \emph{SGD}. Procedura zostaje powtórzona aż do osiągnięcia kryterium wyjścia na danych testujących.

\begin{addmargin}[10mm]{0mm}
\begin{lstlisting}[
    language=Python,
    numbers=left,
    firstnumber=1,
    caption={Algorytm OBD},
    aboveskip=10pt
]
def OBD(self, train_data, test_data):
 nabla_h = [np.zeros(w.shape) for w in self.weights]
 par_number = sum([w.shape[0]*w.shape[1] for w in self.weights])
 test_cost = []
 prev_cost = np.inf
 prev_delta_cost_list = np.full((3,),np.inf)

 for limit in range(int(0.9*par_number)):
   for x, y in train_data:
     delta_nabla_h = self.backpropOBD(x,y)
     nabla_h = [nh + dnh
       for nh, dnh in zip(nabla_h, delta_nabla_h)]

   self.saliencies = [(h * w**2)/(2 * len(train_data))
                                for w, h in zip(self.weights, nabla_h)]
   self.cut_weights(limit+1)
   self.SGD(train_data, 200, 10, 3.0)        
   test_cost.append(self.total_cost(test_data))    
   current_cost = self.total_cost(test_data)            
   prev_delta_cost_list = np.delete(np.insert(
     prev_delta_cost_list, 0, prev_cost - current_cost),-1)
   prev_cost = current_cost
   #stopping rule
   if all(prev_delta_cost_list < self.cost_delta_epsilon/100):
     return train_cost, test_cost 

 return train_cost, test_cost
\end{lstlisting}
\end{addmargin}

\par Niezbędnym dla zrozumienia całego algorytmu OBD jest proces wstecznej propagacji zastosowany dla drugich pochodnych. Jego implementacja zaprezentowana została na Listingu 7 i zawiera wiele podobieństw do wcześniej omawianej funkcji \emph{backprop}. Należy zwrócić tutaj szczególną uwagę na linie 23-24, które odpowiadają równaniu (23). W linii 15 widzimy natomiast wykorzystanie funkcji pomocniczej do wyliczenia warunku granicznego z równania (24), zabieg ten sprzyja czytelności kodu, a z uwagi na mało skomplikowaną formę równania nie powinien stanowić utrudnienia w zrozumieniu całej metody.
\begin{addmargin}[10mm]{0mm}
\begin{lstlisting}[
    language=Python,
    numbers=left,
    firstnumber=1,
    caption={Funkcja wstecznej propagacji drugich pochodnych},
    aboveskip=10pt
]
def backpropOBD(self, x, y):
        
 h_vector = [np.zeros(w.shape) for w in self.weights]
 # feedforward
 activation = x
 activations = [x] 
 zs = []
 for b, w in zip(self.biases, self.weights):
   z = np.dot(w, activation)+b
   zs.append(z)
   activation = sigmoid(z)
   activations.append(activation)

  #wsteczna propagacja drugich pochodnych
  delta2_z = self.boundry_OBD_derivative(zs[-1], y)
  h_vector[-1] = np.dot(delta2_z,
    activations[-2].transpose()**2)
  #iteracja po kolejnych warstwach
  for l in range(2, self.num_layers):
    z = zs[-l]
    sp = sigmoid_prime(z)
    sp2 = sigmoid_second_prime(z)    
    delta2_z =  sp**2 * np.dot(self.weights[-l+1].transpose()**2,
      delta2_z)             
    h_vector[-l] = np.dot(delta2_z,
      activations[-l-1].transpose()**2)
           
  return h_vector
\end{lstlisting}
\end{addmargin}

\par Ostatnia z wykorzystywanych funkcji w procedurze OBD jest funkcja \emph{cut{\_}weights}. Odpowiada ona za przycięcie odpowiedniej ilości wag, ustalanej przez parametr \emph{limit} według posortowanych wartości współczynników skośności \emph{ang. saliencies}. W liniach 2-8 współczynniki są sortowane i w zmiennej \emph{to{\_}cut} zapisywane są indeksy odpowiednich wag, które to następnie zostają przycięte i zamrożone z wykorzystaniem opisywanej wcześniej maski.
\begin{addmargin}[10mm]{0mm}
\begin{lstlisting}[
    language=Python,
    numbers=left,
    firstnumber=1,
    caption={Funkcja redukcji wag},
    aboveskip=10pt
]
def cut_weights(self, limit):
 saliencies_list = []
  for i_layer , saliencies in enumerate(self.saliencies):
   for i_row, row in enumerate(saliencies.tolist()):
    for i_column, value in enumerate(row):
     saliencies_list.append(( [i_layer, i_row, i_column], value))                    
 saliencies_list.sort(key = lambda x: x[1])
 to_cut = [element[0] for element in saliencies_list[:limit]]
 
 self.restore_mask()
 for wt_index in to_cut:
  self.weights[wt_index[0]][wt_index[1]][wt_index[2]] = 0.0
  self.mask[wt_index[0]][wt_index[1]][wt_index[2]] = 0.0
\end{lstlisting}
\end{addmargin}

\vspace{10mm}
\par Lektura powyższego rozdziału stanowiąca uzupełnienie do opisu teoretycznego powinna dać czytelnikowi pełny obraz wykorzystywanych w trakcie eksperymentów struktur. Po zapoznaniu się z elementami implementacji można przejść do rozdziału opisującego wyniki przeprowadzonych eksperymentów. Warto zauważyć, że zrozumienie wszystkich przedstawionych w tym rozdziale szczegółów nie jest warunkiem koniecznym do wyciągnięcia ogólnych wniosków z kolejnego rozdziału lecz na pewno stanowi dużą wartość dodaną dla czytelnika niezaznajomionego z prezentowaną w tej pracy tematyką. 